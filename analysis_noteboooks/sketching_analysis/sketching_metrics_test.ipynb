{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bacf4b13-2f98-4bc3-8d84-c14f25dff728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import scanpy as sc,anndata as ad\n",
    "import squidpy as sq\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,scale\n",
    "from scipy.spatial import distance_matrix, distance\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import BallTree\n",
    "import time\n",
    "import rdata\n",
    "from scipy.sparse import csr_matrix,bsr_matrix,coo_matrix,issparse,lil_matrix,diags\n",
    "from scipy.sparse.linalg import inv\n",
    "import scipy as sp\n",
    "from  scipy.ndimage import gaussian_filter\n",
    "import igraph as ig\n",
    "import glasbey\n",
    "import warnings\n",
    "import cairocffi as cairo\n",
    "from sklearn.metrics import adjusted_rand_score,make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn import preprocessing\n",
    "import libpysal\n",
    "from esda.losh import LOSH\n",
    "from multiprocessing import Pool\n",
    "from mclustpy import mclustpy\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "#from GraphST import GraphST\n",
    "import SEDR\n",
    "#from GraphST.utils import clustering\n",
    "from matplotlib.colors import ListedColormap\n",
    "from esda import Moran\n",
    "from libpysal.weights import KNN\n",
    "from scsampler import scsampler \n",
    "from geosketch import gs\n",
    "from scvalue import SCValue\n",
    "from tqdm import tqdm\n",
    "from fbpca import pca\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.linalg import norm\n",
    "import joblib\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class RASP:\n",
    "    @staticmethod\n",
    "    def build_weights_matrix(adata, n_neighbors=6, beta=2, platform='visium'):\n",
    "        \"\"\"\n",
    "        Build a sparse distance matrix including only the K nearest neighbors, and compute inverse weighting.\n",
    "\n",
    "        Parameters:\n",
    "        - adata: Annotated data object.\n",
    "        - n_neighbors: int - number of nearest neighbors to include.\n",
    "        - beta: weight exponent parameter.\n",
    "        - platform: string - type of platform.\n",
    "\n",
    "        Returns:\n",
    "        - sparse_distance_matrix: csr_matrix - sparse distance matrix of shape (n_samples, n_samples).\n",
    "        \"\"\"\n",
    "        coords = adata.obsm['spatial']\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(coords)\n",
    "        distances, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "        # Build the sparse matrix\n",
    "        data = distances.flatten()\n",
    "        row_indices = np.repeat(np.arange(coords.shape[0]), n_neighbors)\n",
    "        col_indices = indices.flatten()\n",
    "        sparse_distance_matrix = coo_matrix((data, (row_indices, col_indices)), shape=(coords.shape[0], coords.shape[0])).tocsr()\n",
    "\n",
    "        # Remove outliers\n",
    "        temp_matrix = sparse_distance_matrix.tocoo()\n",
    "        percentile_99 = np.percentile(temp_matrix.data, 99)\n",
    "        temp_matrix.data[temp_matrix.data > percentile_99] = 0\n",
    "        sparse_distance_matrix = temp_matrix.tocsr()\n",
    "\n",
    "        # Invert and exponentiate non-zero values\n",
    "        non_zero_values = sparse_distance_matrix.data[sparse_distance_matrix.data > 0]\n",
    "        min_non_zero_value = np.min(non_zero_values) if non_zero_values.size > 0 else 1\n",
    "\n",
    "        if platform == 'visium':\n",
    "            sparse_distance_matrix.setdiag(min_non_zero_value / 2)\n",
    "        else:\n",
    "            sparse_distance_matrix.setdiag(min_non_zero_value)\n",
    "\n",
    "        inverse_sq_data = np.zeros_like(sparse_distance_matrix.data)\n",
    "        inverse_sq_data[sparse_distance_matrix.data > 0] = 1 / (sparse_distance_matrix.data[sparse_distance_matrix.data > 0] ** beta)\n",
    "\n",
    "        inverse_sq_matrix = csr_matrix((inverse_sq_data, sparse_distance_matrix.indices, sparse_distance_matrix.indptr),\n",
    "                                        shape=sparse_distance_matrix.shape)\n",
    "\n",
    "        row_sums = inverse_sq_matrix.sum(axis=1).A1\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        weights = inverse_sq_matrix.multiply(1 / row_sums[:, np.newaxis])\n",
    "\n",
    "        return weights\n",
    "\n",
    "    @staticmethod\n",
    "    def clustering(adata, n_clusters=7, n_neighbors=10, key='X_pca_smoothed', method='mclust'):\n",
    "        \"\"\"\n",
    "        Spatial clustering.\n",
    "\n",
    "        Parameters:\n",
    "        - adata: AnnData object of scanpy package.\n",
    "        - n_clusters: int, optional - The number of clusters. Default is 7.\n",
    "        - n_neighbors: int, optional - The number of neighbors considered during refinement. Default is 15.\n",
    "        - key: string, optional - The key of the learned representation in adata.obsm. Default is 'X_pca_smoothed'.\n",
    "        - method: string, optional - The tool for clustering. Supported tools: 'mclust', 'leiden', 'louvain'.\n",
    "\n",
    "        Returns:\n",
    "        - adata: Updated AnnData object with clustering results.\n",
    "        \"\"\"\n",
    "\n",
    "        if method == 'mclust':\n",
    "            np.random.seed(2020)\n",
    "            import rpy2.robjects as robjects\n",
    "            robjects.r.library(\"mclust\")\n",
    "            import rpy2.robjects.numpy2ri\n",
    "            rpy2.robjects.numpy2ri.activate()\n",
    "            r_random_seed = robjects.r['set.seed']\n",
    "            r_random_seed(2020)\n",
    "            rmclust = robjects.r['Mclust']\n",
    "            res = rmclust(rpy2.robjects.numpy2ri.numpy2rpy(adata.obsm[key]), n_clusters, 'EEE')\n",
    "            mclust_res = np.array(res[-2])\n",
    "            adata.obs[f'RASP_{method}_clusters'] = mclust_res\n",
    "            adata.obs[f'RASP_{method}_clusters'] = adata.obs[f'RASP_{method}_clusters'].astype('int')\n",
    "            adata.obs[f'RASP_{method}_clusters'] = adata.obs[f'RASP_{method}_clusters'].astype('category')\n",
    "\n",
    "        elif method == 'louvain':\n",
    "            adata = RASP.louvain(adata, n_clusters, n_neighbors=n_neighbors, key_added='RASP_louvain_clusters')\n",
    "\n",
    "        elif method == 'leiden':\n",
    "            adata = RASP.leiden(adata, n_clusters, n_neighbors=n_neighbors, key_added='RASP_leiden_clusters')\n",
    "\n",
    "        elif method ==\"walktrap\":\n",
    "                neighbors_graph = adata.obsp['connectivities']\n",
    "                sources, targets = neighbors_graph.nonzero()\n",
    "                weights = neighbors_graph[sources, targets].A1\n",
    "                g = ig.Graph(directed=False)\n",
    "                g.add_vertices(adata.n_obs)\n",
    "                g.add_edges(zip(sources, targets))\n",
    "                g.es['weight'] = weights\n",
    "            \n",
    "                # Perform Walktrap community detection\n",
    "                start_time = time.time()\n",
    "                walktrap = g.community_walktrap(weights='weight')\n",
    "                clusters = walktrap.as_clustering(n=n_clusters)\n",
    "                end_time = time.time()\n",
    "                cluster_time = end_time - start_time\n",
    "                adata.obs[f'RASP_{method}_clusters'] = pd.Categorical(clusters.membership)\n",
    "\n",
    "        elif method == \"KMeans\":\n",
    "            kmeans = KMeans(n_clusters = n_clusters,random_state = 10)\n",
    "            adata.obs[f'RASP_{method}_clusters'] = pd.Categorical(kmeans.fit_predict(adata.obsm['X_pca_smoothed']))\n",
    "\n",
    "        num_clusters = len(set(adata.obs[f'RASP_{method}_clusters']))\n",
    "        palette = glasbey.create_palette(palette_size=num_clusters)\n",
    "        adata.uns[f'RASP_{method}_clusters_colors'] = palette\n",
    "\n",
    "        return adata\n",
    "\n",
    "    @staticmethod\n",
    "    def louvain(adata,n_clusters,n_neighbors = 10,use_rep = 'X_pca_smoothed',key_added = 'RASP_louvain_clusters',random_seed = 2023):\n",
    "        res = RASP.res_search_fixed_clus_louvain(adata, n_clusters, increment=0.1, start = 0.001,random_seed=random_seed)\n",
    "        print(f'resolution is: {res}')\n",
    "        sc.tl.louvain(adata, random_state=random_seed, resolution=res)\n",
    "       \n",
    "        adata.obs[key_added] = adata.obs['louvain']\n",
    "        adata.obs[key_added] = adata.obs[key_added].astype('int')\n",
    "        adata.obs[key_added] = adata.obs[key_added].astype('category')\n",
    "\n",
    "        return adata\n",
    "\n",
    "    @staticmethod\n",
    "    def leiden(adata,n_clusters,n_neighbors = 10,use_rep = 'X_pca_smoothed',key_added = 'RASP_leiden_clusters',random_seed = 2023):\n",
    "        res = RASP.res_search_fixed_clus_leiden(adata, n_clusters, increment=0.1, start = 0.001,random_seed=random_seed)\n",
    "        print(f'resolution is: {res}')\n",
    "        sc.tl.leiden(adata, random_state=random_seed, resolution=res)\n",
    "       \n",
    "        adata.obs[key_added] = adata.obs['leiden']\n",
    "        adata.obs[key_added] = adata.obs[key_added].astype('int')\n",
    "        adata.obs[key_added] = adata.obs[key_added].astype('category')\n",
    "\n",
    "        return adata\n",
    "\n",
    "    @staticmethod\n",
    "    def res_search_fixed_clus_louvain(adata, n_clusters, increment=0.1, start=0.001, random_seed=2023):\n",
    "        \"\"\"\n",
    "        Search for the correct resolution for the Louvain clustering algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - adata: AnnData object containing the data.\n",
    "        - n_clusters: int - The target number of clusters.\n",
    "        - increment: float, optional - The step size for resolution search (default is 0.1).\n",
    "        - start: float, optional - The starting resolution for the search (default is 0.001).\n",
    "        - random_seed: int, optional - Random seed for reproducibility (default is 2023).\n",
    "\n",
    "        Returns:\n",
    "        - float: The largest correct resolution found for the specified number of clusters.\n",
    "        \"\"\"\n",
    "        if increment < 0.0001:\n",
    "            print(\"Increment too small, returning starting value.\")\n",
    "            return start  # Return the initial start value\n",
    "        #keep track of the currect resolution and the largest resolution that is not to large. \n",
    "        largest_correct_res = None\n",
    "        current_res = start\n",
    "        for res in np.arange(start,2,increment):\n",
    "            sc.tl.louvain(adata,random_state = random_seed,resolution = res)\n",
    "            \n",
    "            #increase res tracker to current res\n",
    "            current_res = res\n",
    "\n",
    "            \n",
    "            num_clusters = len(adata.obs['louvain'].unique())\n",
    "            print(f'Resolution: {res} gives cluster number: {num_clusters}')\n",
    "\n",
    "            if num_clusters == n_clusters:\n",
    "                largest_correct_res = res  # Update the largest correct resolution found\n",
    "            \n",
    "            #Check to see if the res resulted in too many clusters! \n",
    "            #break out of loop if we exceed this point. \n",
    "            if num_clusters > n_clusters:\n",
    "                break\n",
    "\n",
    "        \n",
    "        #return correct res if you have one! \n",
    "        if largest_correct_res is not None:\n",
    "            return largest_correct_res\n",
    "\n",
    "        #perform tail end recursion until correct res is found! \n",
    "        else:\n",
    "            return RASP.res_search_fixed_clus_louvain(\n",
    "                adata,\n",
    "                n_clusters,\n",
    "                increment = increment/10,\n",
    "                start = current_res - increment,\n",
    "                random_seed = random_seed)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def res_search_fixed_clus_leiden(adata, n_clusters, increment=0.1, start=0.001, random_seed=2023):\n",
    "        \"\"\"\n",
    "        Search for the correct resolution for the Leiden clustering algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - adata: AnnData object containing the data.\n",
    "        - n_clusters: int - The target number of clusters.\n",
    "        - increment: float, optional - The step size for resolution search (default is 0.1).\n",
    "        - start: float, optional - The starting resolution for the search (default is 0.001).\n",
    "        - random_seed: int, optional - Random seed for reproducibility (default is 2023).\n",
    "\n",
    "        Returns:\n",
    "        - float: The largest correct resolution found for the specified number of clusters.\n",
    "        \"\"\"\n",
    "        if increment < 0.0001:\n",
    "            print(\"Increment too small, returning starting value.\")\n",
    "            return start  # Return the initial start value\n",
    "        #keep track of the currect resolution and the largest resolution that is not to large. \n",
    "        largest_correct_res = None\n",
    "        current_res = start\n",
    "        for res in np.arange(start,2,increment):\n",
    "            sc.tl.leiden(adata,random_state = random_seed,resolution = res)\n",
    "            \n",
    "            #increase res tracker to current res\n",
    "            current_res = res\n",
    "\n",
    "            \n",
    "            num_clusters = len(adata.obs['leiden'].unique())\n",
    "            print(f'Resolution: {res} gives cluster number: {num_clusters}')\n",
    "\n",
    "            if num_clusters == n_clusters:\n",
    "                largest_correct_res = res  # Update the largest correct resolution found\n",
    "            \n",
    "            #now check to see if the res resulted in too many clusters! \n",
    "            #break out of loop if we exceed this point. \n",
    "            if num_clusters > n_clusters:\n",
    "                break\n",
    "\n",
    "        \n",
    "        #return correct res if you have one! \n",
    "        if largest_correct_res is not None:\n",
    "            return largest_correct_res\n",
    "\n",
    "        #perform tail end recursion until correct res is found! \n",
    "        else:\n",
    "            return RASP.res_search_fixed_clus_leiden(\n",
    "                adata,\n",
    "                n_clusters,\n",
    "                increment = increment/10,\n",
    "                start = current_res - increment,\n",
    "                random_seed = random_seed)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def fx_1NN(index, location_in):\n",
    "        \"\"\"\n",
    "        Python equivalent of the fx_1NN function that is called in the loop.\n",
    "        Computes the distance from the point at 'index' to its nearest neighbor.\n",
    "        \"\"\"\n",
    "        distances = cdist([location_in[index]], location_in, 'euclidean')\n",
    "        nearest_neighbor = np.partition(distances, 1)[0, 1]  # 1st closest distance\n",
    "        return nearest_neighbor\n",
    "    @staticmethod\n",
    "    def CHAOS(clusterlabel, location):\n",
    "        matched_location = np.array(location)\n",
    "        clusterlabel = np.array(clusterlabel)\n",
    "        \n",
    "        # Remove NA (None) values\n",
    "        NAs = np.where(pd.isna(clusterlabel))[0]\n",
    "        if len(NAs) > 0:\n",
    "            clusterlabel = np.delete(clusterlabel, NAs)\n",
    "            matched_location = np.delete(matched_location, NAs, axis=0)\n",
    "    \n",
    "        # Standardize the location data\n",
    "        matched_location = scale(matched_location)\n",
    "    \n",
    "        unique_labels = np.unique(clusterlabel)\n",
    "        dist_val = np.zeros(len(unique_labels))\n",
    "        \n",
    "        for count, k in enumerate(unique_labels):\n",
    "            location_cluster = matched_location[clusterlabel == k]\n",
    "            if location_cluster.shape[0] == 1:  # Only one point in cluster\n",
    "                continue\n",
    "    \n",
    "            with Pool(5) as pool:  # Parallel processing with 5 cores\n",
    "                results = pool.starmap(RASP.fx_1NN, [(i, location_cluster) for i in range(location_cluster.shape[0])])\n",
    "            \n",
    "            dist_val[count] = sum(results)\n",
    "        \n",
    "        dist_val = dist_val[~np.isnan(dist_val)]  # Remove any NaN values\n",
    "        return np.sum(dist_val) / len(clusterlabel)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct_gene(adata, \n",
    "                                smoothed_pca_matrix, \n",
    "                                weights,\n",
    "                                gene_name='test', \n",
    "                                quantile_prob=0.001,\n",
    "                                scale = False,\n",
    "                                threshold_method = 'ALRA',\n",
    "                                rank_k = 20):\n",
    "\n",
    " \n",
    "        \"\"\"\n",
    "        Restore true biological zeros while considering excess zeros and apply scaling.\n",
    "        \n",
    "        Parameters:\n",
    "        - adata: AnnData object containing the gene expression data.\n",
    "        - smoothed_pca_one: PCA smoothed data after initial PCA.\n",
    "        - smoothed_pca_two: PCA smoothed data after adding features.\n",
    "        - pca_weights_initial: Weights from the initial PCA.\n",
    "        - pca_weights_final: Weights from the final PCA (optional).\n",
    "        - gene_name: The specific gene for which to restore zeros.\n",
    "        - quantile_prob: The quantile threshold to use for determining biological zeros.\n",
    "        - plot_hist: Bool indicator to output the histogram of the gene expression before and after reconstruction\n",
    "        - scale: Bool indicator to scale values to match original expression\n",
    "        - threshold_method: ALRA or Zero, how to deal with restoration of biological zeros to the imputed data. \n",
    "        \n",
    "        Returns:\n",
    "        - adata: Updated AnnData object with reconstructed zeros.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the original gene expression data\n",
    "        original_data = adata.X.toarray()\n",
    "        indices = range(rank_k)\n",
    "        gene_index = adata.var.index.get_loc(gene_name)\n",
    "        original_expression = original_data[:, gene_index].toarray().flatten() if isinstance(original_data, csr_matrix) else original_data[:, gene_index]\n",
    "    \n",
    "        \n",
    "    \n",
    "            #subset to get rank k reconstruction: \n",
    "        indices = range(rank_k)\n",
    "        smoothed_pca_matrix = smoothed_pca_matrix[:,indices]\n",
    "    \n",
    "        gene_weights = weights[indices, gene_index]\n",
    "        reconstructed_gene_expression = np.dot(smoothed_pca_matrix, gene_weights)\n",
    "    \n",
    "        delta_mean = np.mean(original_expression)\n",
    "        reconstructed_gene_expression += delta_mean\n",
    "    \n",
    "        # Calculate the quantile threshold using absolute value\n",
    "        #note: the ALRA method uses the abs of the quantile and then restores the expression of some cell cells that are non-zero \n",
    "        # from the original expression matrix. This is different than what I am doing which is taking whatever is smaller: the threshold or \n",
    "        # zero. \n",
    "    \n",
    "    \n",
    "        if threshold_method == 'Zero':\n",
    "            threshold_value = np.quantile(reconstructed_gene_expression, quantile_prob)\n",
    "            threshold_value = max(0,threshold_value)\n",
    "        \n",
    "            print(f'Threshold read value: {np.quantile(reconstructed_gene_expression, quantile_prob)}')\n",
    "            \n",
    "            \n",
    "            # Restore the biological zeros based on the excess zeros logic\n",
    "            restored_expression = reconstructed_gene_expression.copy()\n",
    "            print(f'Number of cells below the threshold: {np.sum(restored_expression < threshold_value)}')\n",
    "            print(f'Number of cells below zero: {np.sum(restored_expression < 0)}')\n",
    "        \n",
    "            restored_expression[restored_expression < threshold_value] = 0\n",
    "    \n",
    "            \n",
    "        \n",
    "            #in case negative values remain, set those to zero as well! \n",
    "            #restored_expression[restored_expression < 0] = 0 \n",
    "        \n",
    "            print(f'Number of cells with zero before imputation:{np.sum(original_expression==0)}')\n",
    "            print(f'Number of cells with zero AFTER imputation:{np.sum(restored_expression==0)}')\n",
    "    \n",
    "        if threshold_method == 'ALRA':\n",
    "            threshold_value =  np.abs(np.quantile(reconstructed_gene_expression, quantile_prob))\n",
    "            print(f'Threshold (absolute value for ALRA method): {threshold_value}')\n",
    "            restored_expression = reconstructed_gene_expression.copy()\n",
    "            print(f'Number of cells below the threshold: {np.sum(restored_expression < threshold_value)}')\n",
    "            print(f'Number of cells below zero: {np.sum(restored_expression < 0)}')\n",
    "            restored_expression[restored_expression < threshold_value] = 0\n",
    "            \n",
    "            # Restore original values for Non-Zero entries that were thresholded out\n",
    "            mask_thresholded_to_zero = (reconstructed_gene_expression < threshold_value) & (original_expression > 0)\n",
    "    \n",
    "            #note: the ALRA method restors the original expression here. What I am doing is instead restoring the \n",
    "            #reconstructed expression, as long as it is not zero! \n",
    "            #restored_expression[mask_thresholded_to_zero] = original_expression[mask_thresholded_to_zero]\n",
    "            restored_expression[mask_thresholded_to_zero] = reconstructed_gene_expression[mask_thresholded_to_zero]\n",
    "            print(f'Number of cells restored to original values:{np.sum(mask_thresholded_to_zero != 0)}')\n",
    "            print(f'Number of cells that where negative: {np.sum(reconstructed_gene_expression[mask_thresholded_to_zero]<0)}')\n",
    "    \n",
    "            #finally, set anything that is still negative to zero, should be a very small number of cells! \n",
    "            restored_expression[restored_expression < 0] = 0\n",
    "            \n",
    "        if scale:\n",
    "            \n",
    "    \n",
    "            # Now, perform scaling based on the original and restored values\n",
    "            sigma_1 = np.std(restored_expression[restored_expression > 0])\n",
    "            sigma_2 = np.std(original_expression[original_expression > 0])\n",
    "            mu_1 = np.mean(restored_expression[restored_expression > 0])\n",
    "            mu_2 = np.mean(original_expression[original_expression > 0])\n",
    "        \n",
    "            # Avoid division by zero\n",
    "            if sigma_1 == 0:\n",
    "                sigma_1 = 1e-10  # Or choose to keep restored_expression intact\n",
    "            \n",
    "            # Determine scaling factors\n",
    "            scaling_factor = sigma_2 / sigma_1\n",
    "            offset = mu_2 - (mu_1 * scaling_factor)\n",
    "        \n",
    "            # Apply scaling\n",
    "            restored_expression = restored_expression * scaling_factor + offset\n",
    "        \n",
    "            # If case scaling results in any negative values, turn those to zero as well! \n",
    "            #print(f'Number of cells turned negative after scaling: {np.sum(restored_expression_scaled < 0)}')\n",
    "            restored_expression[restored_expression < 0] = 0\n",
    "            \n",
    "    \n",
    "        # Store the final restored gene expression back into adata\n",
    "        adata.obs['restored_' + gene_name] = restored_expression.flatten()\n",
    "            \n",
    "        return adata\n",
    "\n",
    "\n",
    "#functions: \n",
    "def compute_cell_type_proportions_annoy(subset_adata, full_adata, ground_truth=\"Cell_Type\", k=20, n_trees=10):\n",
    "    \"\"\"\n",
    "    Computes the proportion of cell types in the neighborhood of each cell using Annoy.\n",
    "\n",
    "    Parameters:\n",
    "    - subset_adata: AnnData object with spatial coordinates for the subset dataset in `subset_adata.obsm`\n",
    "    - full_adata: AnnData object representing the full dataset with all possible cell types\n",
    "    - k: Number of nearest neighbors to consider\n",
    "    - n_trees: Number of trees for Annoy (higher = better accuracy but slower)\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame of shape (n_cells, n_cell_types) with proportions of each cell type in its neighborhood, aligned with full dataset cell types.\n",
    "    \"\"\"\n",
    "    # Extract spatial coordinates from subset adata\n",
    "    coords = subset_adata.obsm[\"spatial\"]\n",
    "    n_cells, n_dims = coords.shape\n",
    "\n",
    "    # Build Annoy index\n",
    "    annoy_index = AnnoyIndex(n_dims, metric='euclidean')\n",
    "    for i in range(n_cells):\n",
    "        annoy_index.add_item(i, coords[i])\n",
    "    annoy_index.build(n_trees)  # Build the index\n",
    "\n",
    "    # Extract cell type labels from full adata\n",
    "    full_cell_types = np.array(full_adata.obs[ground_truth])\n",
    "    full_unique_types = np.unique(full_cell_types)  # Full unique cell types\n",
    "    type_to_idx = {t: i for i, t in enumerate(full_unique_types)}  # Mapping cell type to column index\n",
    "\n",
    "    # Initialize output matrix\n",
    "    proportions = np.zeros((n_cells, len(full_unique_types)))\n",
    "\n",
    "    # Extract cell type labels from subset adata\n",
    "    subset_cell_types = np.array(subset_adata.obs[ground_truth])\n",
    "\n",
    "    # Compute kNN for each cell in subset adata\n",
    "    for i in range(n_cells):\n",
    "        neighbors = annoy_index.get_nns_by_item(i, k+1)[1:]  # Exclude self\n",
    "        neighbor_types = subset_cell_types[neighbors]  # Get cell types of kNN\n",
    "        # Count occurrences of each cell type\n",
    "        for t in neighbor_types:\n",
    "            proportions[i, type_to_idx[t]] += 1\n",
    "        proportions[i] /= k  # Normalize to proportions\n",
    "\n",
    "    # Convert to DataFrame with full unique cell types as columns\n",
    "    prop_df = pd.DataFrame(proportions, columns=full_unique_types, index=subset_adata.obs.index)\n",
    "    return prop_df\n",
    "\n",
    "\n",
    "def get_leverage_index(adata,fraction = 0.1,score_column=\"gene_score\",seed = 0):\n",
    "    #print(\"running leverage sampling\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if score_column not in adata.obs:\n",
    "        raise ValueError(f\"Column '{score_column}' not found in adata.obs\")\n",
    "\n",
    "    scores = adata.obs[score_column].values\n",
    "    # scores = np.clip(scores, a_min=0, a_max=None)  # Ensure no negative values\n",
    "    probabilities = scores / scores.sum()  # Normalize to get probabilities\n",
    "\n",
    "    num_cells = adata.n_obs\n",
    "    sample_size = int(num_cells * fraction)\n",
    "\n",
    "    sampled_indices = np.random.choice(adata.n_obs, size=sample_size, replace=False, p=probabilities)\n",
    "    return sorted(list(sampled_indices))\n",
    "\n",
    "def create_leverage_index_dataframe(adata, fraction=0.1, num_seeds=10, start_seed=0):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame where each column contains the sorted indices from uniform_index\n",
    "    for a different random seed.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object to sample from\n",
    "        fraction (float): Fraction of cells to sample\n",
    "        num_seeds (int): Number of different seeds to use\n",
    "        start_seed (int): Starting seed value\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with columns named 'seed_{seed}' containing sorted indices\n",
    "    \"\"\"\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    # Run uniform_index for each seed\n",
    "    for seed in range(start_seed, start_seed + num_seeds):\n",
    "        column_name = f'seed_{seed}'\n",
    "        indices = get_leverage_index(adata, fraction=fraction, seed=seed)\n",
    "        results[column_name] = indices\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    # Note: Columns might have different lengths, so we'll use a different approach\n",
    "    df_dict = {}\n",
    "    \n",
    "    # Find the maximum length of any index list\n",
    "    max_length = max(len(indices) for indices in results.values())\n",
    "    \n",
    "    # Pad shorter lists with NaN values\n",
    "    for column_name, indices in results.items():\n",
    "        # Pad with NaN if needed\n",
    "        padded_indices = indices + [np.nan] * (max_length - len(indices))\n",
    "        df_dict[column_name] = padded_indices\n",
    "    \n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame(df_dict)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def uniform_sample_adata(adata, fraction=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Uniformly samples a fraction of cells from an AnnData object.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object\n",
    "    - fraction: float, fraction of cells to sample (default is 10%)\n",
    "    - random_state: int, random seed for reproducibility (default is None)\n",
    "    \n",
    "    Returns:\n",
    "    - AnnData object with sampled cells\n",
    "    \"\"\"\n",
    "    print(\"running uniform sampling\")\n",
    "    np.random.seed(random_state)\n",
    "    num_cells = adata.n_obs  # Total number of cells\n",
    "    sample_size = int(num_cells * fraction)  # Compute number of cells to sample\n",
    "\n",
    "    sampled_indices = np.random.choice(adata.obs.index, size=sample_size, replace=False)\n",
    "    \n",
    "    return adata[sampled_indices].copy()  # Return a new AnnData object\n",
    "\n",
    "\n",
    "def compare_cell_type_proportions(matrix1, matrix2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity, Jensen-Shannon Divergence (JSD), Mean Squared Error (MSE),\n",
    "    and Frobenius norm between two cell type proportion matrices.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix1: pandas DataFrame (n_cells x n_cell_types), first cell type proportion matrix\n",
    "    - matrix2: pandas DataFrame (n_cells x n_cell_types), second cell type proportion matrix\n",
    "\n",
    "    Returns:\n",
    "    - results: dict with 'mean_cosine_similarity', 'mean_jsd', 'mse', and 'frobenius_norm'\n",
    "    \"\"\"\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"Both matrices must have the same shape.\")\n",
    "    \n",
    "    # Initialize results with NaN\n",
    "    results = {\n",
    "        \"mean_cosine_similarity\": np.nan,\n",
    "        \"mean_jsd\": np.nan,\n",
    "        \"mse\": np.nan,\n",
    "        \"frobenius_norm\": np.nan\n",
    "    }\n",
    "\n",
    "    # Compute Cosine Similarity (mean across all cells) and handle memory issues\n",
    "    try:\n",
    "        cos_sim = cosine_similarity(matrix1, matrix2)\n",
    "        results[\"mean_cosine_similarity\"] = np.mean(np.diag(cos_sim))  # Take mean of diagonal elements (self-similarity)\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError encountered during cosine similarity calculation. Calculating manually.\")\n",
    "        try:\n",
    "            cos_sim_values = []\n",
    "            for i in range(matrix1.shape[0]):\n",
    "                vec1 = matrix1.iloc[i].values\n",
    "                vec2 = matrix2.iloc[i].values\n",
    "                dot_product = np.dot(vec1, vec2)\n",
    "                norm1 = np.linalg.norm(vec1)\n",
    "                norm2 = np.linalg.norm(vec2)\n",
    "                \n",
    "                if norm1 > 0 and norm2 > 0:\n",
    "                    cos_sim_values.append(dot_product / (norm1 * norm2))\n",
    "                else:\n",
    "                    cos_sim_values.append(0)  # Handle zero vector case\n",
    "            \n",
    "            results[\"mean_cosine_similarity\"] = np.mean(cos_sim_values)\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during manual cosine similarity calculation: {e}\")\n",
    "\n",
    "    # Compute Jensen-Shannon Divergence (JSD) and handle memory issues\n",
    "    try:\n",
    "        jsd_values = [jensenshannon(matrix1.iloc[i], matrix2.iloc[i]) for i in range(len(matrix1))]\n",
    "        results[\"mean_jsd\"] = np.mean(jsd_values)\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError encountered during JSD calculation. Returning NaN.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during JSD calculation: {e}\")\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) and handle memory issues\n",
    "    try:\n",
    "        results[\"mse\"] = mean_squared_error(matrix1, matrix2)\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError encountered during MSE calculation. Returning NaN.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during MSE calculation: {e}\")\n",
    "\n",
    "    # Compute Frobenius Norm and handle memory issues\n",
    "    try:\n",
    "        results[\"frobenius_norm\"] = norm(matrix1.values - matrix2.values, 'fro')\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError encountered during Frobenius Norm calculation. Returning NaN.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Frobenius Norm calculation: {e}\")\n",
    "\n",
    "    return results\n",
    "def compute_neighborhood_metrics(adata, fraction=0.1, seed=0, k=20, ground_truth=\"Cell_Type\", n_trees=10):\n",
    "    \"\"\"\n",
    "    Evaluates four sampling methods by computing cell type proportions and comparing them to the full dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object\n",
    "    - fraction: float, fraction of cells to sample\n",
    "    - seed: int, random seed for reproducibility\n",
    "    - k: int, number of nearest neighbors for Annoy\n",
    "    - ground_truth: str, column in `adata.obs` containing cell type labels\n",
    "    - n_trees: int, number of trees for Annoy index\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing similarity metrics for each sampling method\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    # Compute full dataset cell type proportions\n",
    "    print(\"Computing cell type proportions for full dataset...\")\n",
    "    cell_type_proportions_full = compute_cell_type_proportions_annoy(adata, adata,ground_truth = ground_truth)\n",
    "\n",
    "    # Generate sampled indices using different methods\n",
    "    print(\"Generating sampled indices...\")\n",
    "    sampled_indices = sampling_methods_get_index(adata,fraction=fraction, seed=seed)\n",
    "    results = {}\n",
    "\n",
    "    for method, indices in sampled_indices.items():\n",
    "        print(f\"Processing {method}...\")\n",
    "        # Compute cell type proportions for subsampled data\n",
    "        sampled_adata = adata[indices]\n",
    "        cell_type_proportions_sub = compute_cell_type_proportions_annoy(sampled_adata, adata,ground_truth = ground_truth)\n",
    "\n",
    "        # Extract corresponding subset from full dataset\n",
    "        cell_type_proportions_full_partial = cell_type_proportions_full.loc[adata.obs.index[indices]]\n",
    "\n",
    "        # Compute similarity metrics\n",
    "        metrics = compare_cell_type_proportions(cell_type_proportions_sub, cell_type_proportions_full_partial)\n",
    "        \n",
    "        # Store metrics\n",
    "        results[method] = metrics\n",
    "\n",
    "    # Convert results dictionary to DataFrame\n",
    "    results_df = pd.DataFrame.from_dict(results,orient = 'index').reset_index()\n",
    "    results_df.rename(columns = {results_df.columns[0]:'method'},inplace = True)\n",
    "\n",
    "    print(f\"Evaluation completed in {time.time() - start_time:.2f} seconds.\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "def load_csv_by_fraction(directory, target_fraction= 0):\n",
    "    \"\"\"\n",
    "    Walks through a directory to find and load a CSV file with a matching fraction in its name.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files\n",
    "        target_fraction (float): The fraction to match in the filename (between 0.0 and 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the CSV data, or None if no matching file is found\n",
    "    \"\"\"\n",
    "    # Convert target_fraction to string for comparison\n",
    "    target_str = str(target_fraction)\n",
    "    \n",
    "    # Pattern to match index_fraction.csv files\n",
    "    pattern = re.compile(r'index_(0?\\.\\d+)\\.csv$')\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            match = pattern.match(file)\n",
    "            if match:\n",
    "                file_fraction = match.group(1)\n",
    "                \n",
    "                # Compare fractions (accounting for different string representations)\n",
    "                if float(file_fraction) == float(target_fraction):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"Found matching file: {file_path}\")\n",
    "                    return pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"No CSV file found with fraction {target_fraction}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def generate_uniform_index_df(adata, fraction=0.1, random_seeds = range(10)):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame containing indices of uniformly sampled subsets of the AnnData object.\n",
    "\n",
    "    :param adata: The AnnData object to sample from.\n",
    "    :param fraction: The fraction of data to sample in each iteration.\n",
    "    :param random_seeds: A list of random seeds for reproducibility in each sampling iteration.\n",
    "    :return: A DataFrame where each column represents the indices of a sampled subset.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not 0 < fraction <= 1:\n",
    "        raise ValueError(\"The fraction must be between 0 and 1.\")\n",
    "    \n",
    "    num_samples = len(adata)\n",
    "    num_subsamples = len(random_seeds)  # Should be 10 based on your example\n",
    "    \n",
    "    # DataFrame to store index results\n",
    "    uniform_index_df = pd.DataFrame()\n",
    "    \n",
    "    for i, seed in enumerate(random_seeds):\n",
    "        # Set the random seed\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Determine the number of samples for this fraction\n",
    "        sample_size = int(num_samples * fraction)\n",
    "        \n",
    "        # Sample indices\n",
    "        sampled_indices = sorted(np.random.choice(adata.n_obs, size=sample_size, replace=False))\n",
    "        \n",
    "        # Add sampled indices to DataFrame as a new column\n",
    "        uniform_index_df[f'seed_{i}'] = pd.Index(sampled_indices)\n",
    "    \n",
    "    return uniform_index_df\n",
    "\n",
    "\n",
    "def partial_hausdorff_distance_annoy(array1, array2, q=1e-4, metric='euclidean', n_trees=10):\n",
    "    \"\"\"\n",
    "    Compute the partial Hausdorff distance from array1 to array2 using Annoy for approximation.\n",
    "    \n",
    "    Parameters:\n",
    "    - array1: numpy.ndarray, shape (n, d)\n",
    "        The first array of points.\n",
    "    - array2: numpy.ndarray, shape (m, d)\n",
    "        The second array of points.\n",
    "    - q: float, optional\n",
    "        The parameter for the partial Hausdorff distance, should be between 0 and 1.\n",
    "    - metric: string, optional\n",
    "        The metric to use for distance calculations. Annoy supports 'euclidean' and 'angular'.\n",
    "    - n_trees: int, optional\n",
    "        Number of trees to use for Annoy's index construction.\n",
    "    \n",
    "    Returns:\n",
    "    - float\n",
    "        The estimated partial Hausdorff distance between array1 and array2.\n",
    "    \"\"\"\n",
    "    # Build Annoy index for array2\n",
    "    num_features = array2.shape[1]\n",
    "    annoy_index = AnnoyIndex(num_features, metric)\n",
    "    for i, vector in enumerate(array2):\n",
    "        annoy_index.add_item(i, vector)\n",
    "    print(\"building Annoy index\")\n",
    "    start = time.time()\n",
    "    annoy_index.build(n_trees)\n",
    "    end = time.time()\n",
    "    print(f\"Annoy index took {end-start:.4f} seconds to build\")\n",
    "\n",
    "    # Query for nearest neighbors\n",
    "    print(\"finding nearest neighbors\")\n",
    "    start = time.time()\n",
    "    distances = []\n",
    "    for point in array1:\n",
    "        nearest_idx, dist = annoy_index.get_nns_by_vector(point, 1, include_distances=True)\n",
    "        distances.append(dist[0])\n",
    "    end = time.time()\n",
    "    print(f\"ANN distance calculation took: {end-start:.4f} seconds\")\n",
    "\n",
    "    # Sort all computed distances\n",
    "    distances.sort()\n",
    "    # Determine the index of the Kth largest value for partial Hausdorff\n",
    "    K = int(np.floor((1 - q) * len(distances)))\n",
    "    # Handle edge cases and return the Kth largest value\n",
    "    return distances[K-1] if K > 0 else distances[0]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_full_pca(path='/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/processed_data/full_pca_20comps.joblib'):\n",
    "    \"\"\"Load PCA object from disk once.\"\"\"\n",
    "    return joblib.load(path)\n",
    "\n",
    "def compute_full_metrics(adata,\n",
    "\tfraction = 0.1,\n",
    "\tseed = 0, \n",
    "\tbase_directory = \"/test/dir\", \n",
    "\tground_truth = 'celltype',\n",
    "\tknn_for_neighborhood_analysis = 10,\n",
    "\tn_neighbors_for_clustering = 10,\n",
    "\tcluster_algorithm = 'louvain',n_clusters = 15):\n",
    "\n",
    "\n",
    "    method_index_dict = {\n",
    "        \n",
    "        \"geo_transcriptomic\":\"geo_transcriptomic_index\",\n",
    "        \"leverage\":\"leverage_index\",\n",
    "        \"scsampler_transcriptomic\":\"scsampler_transcriptomic_index\",\n",
    "        \"uniform\":\"uniform_index\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    results = {}\n",
    "    full_pca = adata.obsm['X_pca']\n",
    "\n",
    "    # full_pca = PCA(n_components = 20, svd_solver = 'randomized',random_state = 2024)\n",
    "    # full_pca.fit(adata.X.toarray())\n",
    "    \n",
    "    #cell_type_proportions_full = compute_cell_type_proportions_annoy(adata, adata,ground_truth = ground_truth,k = knn_for_neighborhood_analysis)\n",
    "    print('subsetting')\n",
    "    for method, child_dir in method_index_dict.items():\n",
    "        if child_dir != \"uniform_index\":\n",
    "            index_df = load_csv_by_fraction(os.path.join(base_directory,child_dir),target_fraction = fraction)\n",
    "            #sort the columns for easier access next time: \n",
    "            index_df = index_df.reindex(columns = sorted(index_df.columns))\n",
    "        elif child_dir == \"uniform_index\": \n",
    "            index_df = generate_uniform_index_df(adata,fraction = fraction,random_seeds = range(10))\n",
    "        if method not in results:\n",
    "            results[method] = {}\n",
    "        \n",
    "        \n",
    "        column_name = f\"seed_{seed}\"\n",
    "        if column_name in index_df.columns:\n",
    "        \tindex_values = index_df[column_name].values\n",
    "        \tsampled_adata = adata[index_values]\n",
    "\n",
    "        #next we will iterate over the rows of the dataframe to subsample. \n",
    "        #for column_name, index_values in index_df.items():\n",
    "    \n",
    "            #sampled_adata = adata[index_values.values]\n",
    "    \n",
    "            #neighborhood metrics: \n",
    "            # cell_type_proportions_sub = compute_cell_type_proportions_annoy(sampled_adata, adata,ground_truth = ground_truth,k = knn_for_neighborhood_analysis)\n",
    "            # cell_type_proportions_full_partial = cell_type_proportions_full.loc[adata.obs.index[index_values.values]]\n",
    "            # metrics = compare_cell_type_proportions(cell_type_proportions_sub, cell_type_proportions_full_partial)\n",
    "            \n",
    "            #now for the metrics: \n",
    "            #compute the cell type metrics \n",
    "            #comput the two distance metrics! \n",
    "        print(\"hausdorff calculations\")\n",
    "        transcriptomic_distance=partial_hausdorff_distance_annoy(adata.X.toarray(), sampled_adata.X.toarray(), q=1e-4)\n",
    "        coord_distance = partial_hausdorff_distance_annoy(adata.obsm['spatial'], sampled_adata.obsm['spatial'], q=1e-4)\n",
    "        \n",
    "        results[method]['transcriptomic_distance'] = transcriptomic_distance\n",
    "        results[method]['coord_distance'] = coord_distance\n",
    "                #now we need to calcualte 2 ARIs: The ARI on the full dataset PCA solution (subset)\n",
    "        # and the ARI on the recomputed PCA solution. \n",
    "        #first lets do the original PCA clustering: \n",
    "        #note: only calculate the ARI values if seed==0, otherwise we won't need to recompute it. \n",
    "        #add nans to the ARi otherwise: \n",
    "        #seed = int(column_name.replace(\"seed_\",\"\"))\n",
    "        print(\"ARI computation\")\n",
    "        if seed ==0:\n",
    "            # sc.pp.neighbors(sampled_adata, n_neighbors=n_neighbors_for_clustering, use_rep='X_pca')\n",
    "            # sampled_adata = RASP.clustering(sampled_adata, n_clusters=n_clusters, n_neighbors=n_neighbors_for_clustering, key='X_pca', method=cluster_algorithm)\n",
    "    \n",
    "            # # Get true and predicted labels\n",
    "            ground_truth_labels = sampled_adata.obs[ground_truth].astype(str)\n",
    "            # labels = sampled_adata.obs[f'RASP_{cluster_algorithm}_clusters'].astype(str)\n",
    "    \n",
    "            # # Compute ARI\n",
    "            # ari_original_pca = adjusted_rand_score(ground_truth_labels, labels)\n",
    "            # results[method]['ari_original_pca'] = ari_original_pca\n",
    "    \n",
    "            #now we will re-compute the PCA and re-cluster. \n",
    "            pca_model_sketch = PCA(n_components=20, svd_solver='randomized', random_state=2024)\n",
    "    \n",
    "            pca_data = pca_model_sketch.fit_transform(sampled_adata.X.toarray())\n",
    "            sampled_adata.obsm['X_pca_recomputed'] = pca_data\n",
    "            sc.pp.neighbors(sampled_adata, n_neighbors=n_neighbors_for_clustering, use_rep='X_pca_recomputed')\n",
    "            sampled_adata = RASP.clustering(sampled_adata, n_clusters=n_clusters, n_neighbors=n_neighbors_for_clustering, key='X_pca_recomputed', method=cluster_algorithm)\n",
    "            labels = sampled_adata.obs[f'RASP_{cluster_algorithm}_clusters'].astype(str)\n",
    "            ari_recomputed_pca = adjusted_rand_score(ground_truth_labels, labels)\n",
    "            results[method]['ari_recomputed_pca'] = ari_recomputed_pca\n",
    "        \n",
    "        print(\"PCA distances\")\n",
    "        #Now for the PCA distances \n",
    "        S = sampled_adata.X.toarray()\n",
    "        # full_pca = PCA(n_components = 20, svd_solver = 'randomized',random_state = 2024)\n",
    "        # full_pca.fit(adata.X.toarray())\n",
    "        full_pca = get_full_pca()\n",
    "        P_f = full_pca.transform(S)\n",
    "        # Project S onto PCs of sampled data (sampled_adata)\n",
    "        sketch_pca = PCA(n_components=20, svd_solver='randomized', random_state=2024)\n",
    "        sketch_pca.fit(S)\n",
    "        P_s = sketch_pca.transform(S)\n",
    "\n",
    "        #Step 5: compute euclidean distance between rows of P_f and P_s\n",
    "        distances = np.linalg.norm(P_f-P_s,axis = 1)\n",
    "        mean_distance = np.mean(distances)\n",
    "        median_distance = np.median(distances)\n",
    "\n",
    "        results[method]['pca_mean_diff'] = mean_distance\n",
    "        results[method]['pca_median_diff'] = median_distance\n",
    "\n",
    "\n",
    "        #save the other function parameters as well: seed, k, and fraction \n",
    "        results[method]['knn_for_neighborhood_analysis'] = knn_for_neighborhood_analysis\n",
    "        results[method]['seed'] = seed\n",
    "        results[method]['fraction'] = fraction\n",
    "        # Convert results dictionary to DataFrame\n",
    "    results_df = pd.DataFrame.from_dict(results,orient = 'index').reset_index()\n",
    "    results_df.rename(columns = {results_df.columns[0]:'method'},inplace = True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0803717a-4081-40b7-94b7-fb49159ec57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/processed_data\"\n",
    "adata = sc.read_h5ad(os.path.join(directory,\"processed_data_8um.h5ad\"))\n",
    "\n",
    "\n",
    "#re-run PCA for now: \n",
    "# pca_model = PCA(n_components=20, svd_solver='randomized', random_state=2024)\n",
    "# pca_data = pca_model.fit_transform(adata.X.toarray())\n",
    "# adata.obsm['X_pca'] = pca_data\n",
    "\n",
    "gene_scores_df = pd.read_csv('/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/processed_data/leverage_scores.csv')\n",
    "adata.obs['gene_score'] = gene_scores_df['leverage_score'].values\n",
    "\n",
    "parent_dir = '/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/sketching_test/index'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_directory = '/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/sketching_test/index'\n",
    "ground_truth = 'louvain'\n",
    "knn_for_neighborhood_analysis = 10\n",
    "n_neighbors_for_clustering = 10\n",
    "cluster_algorithm = 'louvain'\n",
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a31ca56c-2dfa-4dd2-b8a4-0e7baa7d424c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs  n_vars = 393434  19059\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'location_id', 'region', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'n_genes', 'leiden', 'louvain', 'gene_score'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'hvg', 'leiden', 'leiden_colors', 'log1p', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'spatialdata_attrs', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764c5898-f210-4fdc-81e7-cce6aea5bb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(adata.var['highly_variable'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be71fdc-55f9-4624-956f-267decbe9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, n_top_genes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0555e5d0-a809-40f1-8c75-6f17096ea6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs  n_vars = 393434  5000\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'location_id', 'region', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'n_genes', 'leiden', 'louvain', 'gene_score'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'hvg', 'leiden', 'leiden_colors', 'log1p', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'spatialdata_attrs', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save a version that is just the top 50 HVG: \n",
    "adata_hvg = adata[:, adata.var['highly_variable']].copy()\n",
    "adata_hvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1f3ca97-b9f2-4da2-bb92-26bf1a0d68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save: \n",
    "adata_hvg.write_h5ad(\"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/processed_data/processed_data_8um_hvg.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032911d7-8f2b-49db-a7c9-5c3325d1cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction = 0.01\n",
    "# seed = 0\n",
    "\n",
    "\n",
    "\n",
    "# print(\"starting the computation:\")\n",
    "# out_df = compute_full_metrics(adata_hvg,\n",
    "# \tfraction = fraction, \n",
    "# \tseed = seed,\n",
    "# \tbase_directory =base_directory, \n",
    "# \tground_truth = ground_truth,\n",
    "# \tknn_for_neighborhood_analysis = knn_for_neighborhood_analysis,\n",
    "# \tn_neighbors_for_clustering = n_neighbors_for_clustering,\n",
    "# \tcluster_algorithm = cluster_algorithm,\n",
    "# \tn_clusters = n_clusters)\n",
    "\n",
    "\n",
    "# out_df.to_csv(f\"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/sketching_test/combined_metrics/jupyter/normal_metrics_seed_{seed}_{fraction}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bdc3660-3781-41bb-afc4-c126dbcd0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new computation: \n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "813fff43-ccc6-4045-9a14-41fd66809e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_hausdorff_distance_annoy_faster(\n",
    "    array1,\n",
    "    array2,\n",
    "    q         = 1e-4,\n",
    "    metric    = 'euclidean',\n",
    "    n_trees   = 10,\n",
    "    #search_k  = None,\n",
    "    n_jobs    = 1,\n",
    "    verbose   = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the partial Hausdorff distance from array1 -> array2 using Annoy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array1 : (N, d) array\n",
    "        Source points.\n",
    "    array2 : (M, d) array\n",
    "        Target points.\n",
    "    q : float, optional\n",
    "        We return the floor((1-q)*N)-th smallest nearestneighbor distance.\n",
    "    metric : {'euclidean','angular'}\n",
    "        Metric for Annoy.\n",
    "    n_trees : int\n",
    "        Number of trees to build in Annoy.\n",
    "    search_k : int or None\n",
    "        Overrides AnnoyIndex.get_nns_by_vector(..., search_k=). \n",
    "        Default None  use Annoy default.\n",
    "    n_jobs : int\n",
    "        Number of parallel workers for querying.  1 = no parallelism.\n",
    "    verbose : bool\n",
    "        Print timing info if True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The partial Hausdorff distance from array1 to array2.\n",
    "    \"\"\"\n",
    "    N, d = array1.shape\n",
    "    M, d2 = array2.shape\n",
    "    assert d == d2, \"array1 and array2 must have same number of columns\"\n",
    "    # 1) build index\n",
    "    if verbose: \n",
    "        print(f\" Building Annoy index ({M} points in {d} dims, {n_trees} trees)...\", end=' ')\n",
    "        t0 = time.time()\n",
    "\n",
    "    idx = AnnoyIndex(d, metric)\n",
    "    for i, v in enumerate(array2):\n",
    "        idx.add_item(i, v)\n",
    "    idx.build(n_trees)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"done in {time.time() - t0:.3f}s\")\n",
    "\n",
    "    # 2) define a singlepoint query\n",
    "    def _nn_dist(pt):\n",
    "        # get_nns_by_vector returns ([indices], [dists])\n",
    "        return idx.get_nns_by_vector(pt, 1, include_distances=True)[1][0]\n",
    "\n",
    "    # 3) gather all N nearestneighbor distances\n",
    "    if verbose:\n",
    "        print(f\" Querying {N} points (n_jobs={n_jobs})...\", end=' ')\n",
    "        t0 = time.time()\n",
    "\n",
    "    if n_jobs == 1:\n",
    "        # pure Python loop but prealloc NumPy\n",
    "        dists = np.empty(N, dtype=np.float32)\n",
    "        for i, pt in enumerate(array1):\n",
    "            dists[i] = _nn_dist(pt)\n",
    "    else:\n",
    "        # joblib parallel\n",
    "        dists = np.array(\n",
    "            Parallel(n_jobs=n_jobs)(\n",
    "                delayed(_nn_dist)(pt) for pt in array1\n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"done in {time.time() - t0:.3f}s\")\n",
    "\n",
    "    # 4) extract the floor((1-q)*N)-th smallest element in O(N) average time\n",
    "    if q <= 0:\n",
    "        return dists.max()\n",
    "    if q >= 1:\n",
    "        return dists.min()\n",
    "\n",
    "    K = int(np.floor((1.0 - q) * N))\n",
    "    if K <= 1:\n",
    "        return dists.min()\n",
    "    # np.partition places the K-1 smallest elements in dists[:K]\n",
    "    # so dists[K-1] is the K-th smallest in the full array.\n",
    "    kth = np.partition(dists, K-1)[K-1]\n",
    "    return float(kth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "838f5d84-e0e9-4963-a46c-3811cef0f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matching file: /dartfs-hpc/rc/lab/F/FrostH/members/igingerich/public_data/visium_HD_brain/sketching_test/index/geo_transcriptomic_index/index_0.01.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "index_df = load_csv_by_fraction(os.path.join(base_directory,\"geo_transcriptomic_index\"),target_fraction = 0.01)\n",
    "        #sort the columns for easier access next time: \n",
    "index_df = index_df.reindex(columns = sorted(index_df.columns))\n",
    "    \n",
    "    \n",
    "    \n",
    "column_name = \"seed_0\"\n",
    "if column_name in index_df.columns:\n",
    "    index_values = index_df[column_name].values\n",
    "    sampled_adata = adata_hvg[index_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11a01c55-8946-4092-92b2-dfe67fa1a11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs  n_vars = 3934  5000\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'location_id', 'region', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'n_genes', 'leiden', 'louvain', 'gene_score'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'hvg', 'leiden', 'leiden_colors', 'log1p', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'spatialdata_attrs', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6953b8c-57c8-4cb1-b512-94425baf341d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs  n_vars = 50000  19059\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'location_id', 'region', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'n_genes', 'leiden', 'louvain', 'gene_score'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'hvg', 'leiden', 'leiden_colors', 'log1p', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'spatialdata_attrs', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata[0:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "751a31a6-fcaf-408f-83b8-1c6c3ac1810e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m dist\u001b[38;5;241m=\u001b[39mpartial_hausdorff_distance_annoy(\u001b[43madata_hvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,sampled_adata\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mtoarray(), q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/scipy/sparse/_compressed.py:1181\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     y \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1180\u001b[0m M, N \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39m_swap(x\u001b[38;5;241m.\u001b[39m_shape_as_2d)\n\u001b[0;32m-> 1181\u001b[0m \u001b[43mcsr_todense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dist=partial_hausdorff_distance_annoy(adata_hvg.X.toarray(),sampled_adata.X.toarray(), q=1e-4)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b379774-60c6-4a23-b59a-dc4749db088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building Annoy index (3934 points in 5000 dims, 10 trees)... done in 1.603s\n",
      " Querying 393434 points (n_jobs=8)... "
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/externals/loky/backend/queues.py\", line 159, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/externals/loky/backend/reduction.py\", line 215, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/externals/loky/backend/reduction.py\", line 208, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\nTypeError: cannot pickle 'annoy.Annoy' object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dist\u001b[38;5;241m=\u001b[39m\u001b[43mpartial_hausdorff_distance_annoy_faster\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata_hvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampled_adata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 73\u001b[0m, in \u001b[0;36mpartial_hausdorff_distance_annoy_faster\u001b[0;34m(array1, array2, q, metric, n_trees, n_jobs, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m         dists[i] \u001b[38;5;241m=\u001b[39m _nn_dist(pt)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# joblib parallel\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     dists \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m---> 73\u001b[0m         \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_nn_dist\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray1\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     76\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/dartfs-hpc/rc/lab/F/FrostH/members/igingerich/envs/miniconda3/envs/r-kernel/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "dist=partial_hausdorff_distance_annoy_faster(adata_hvg.X.toarray(),sampled_adata.X.toarray(), q=1e-4,n_jobs= 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fd785-8785-48bf-ad73-bd478176da10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
